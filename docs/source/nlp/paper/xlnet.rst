======
XLnet
======

기존 모델과의 차별성
=================

AR의 경우 문맥을 양방향(bidirectional)으로 볼 수 없는 태생적 한계를 지닙니다.


BERT로 대표되는 AE는 양방향 모델입니다. 그림 2처럼 마스크 단어를 예측할 때 앞뒤 문맥을 모두 살피기 때문입니다. 이 덕분에 각종 다운스트림 태스크에서 BERT가 상당 기간 절대 강자로 군림할 수 있었습니다. 하지만 AE 역시 단점이 없지 않습니다. 가장 큰 문제는 마스킹 처리한 토큰들을 서로 독립(independent)이라고 가정한다는 점입니다. 이 경우 마스킹 토큰들 사이에 의존 관계(dependency)를 따질 수 없게 됩니다.


Yang et al. (2019)는 AR과 AE 모델의 한계를 극복하기 위해 퍼뮤테이션 언어모델(permutaion language model)을 제안했습다. 토큰을 랜덤으로 셔플한 뒤 그 뒤바뀐 순서가 마치 원래 그랬던 것인 양 언어모델을 학습하는 기법이다. 그림 4는 발 없는 말이 천리 간다를 퍼뮤테이션 언어모델로 학습하는 예시입니다. 그림 4를 보면 모델은 ‘없는, 이, 말, 발, 간다’를 입력받아 시퀀스의 마지막 단어인 ‘천리’를 예측한다.

위 내용은 추후 정리가 필요하다.


Reference
==========

* `ratsgo's blog <https://ratsgo.github.io/natural%20language%20processing/2019/09/11/xlnet/>`_
