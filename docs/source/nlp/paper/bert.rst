=====
BERT
=====

BERT (Bidirectional Encoder Representations from Transformer)의 아키텍처는 "Attention is all you need"에서 소개된 Transformer를 사용하지만, pre-training과 fine-tuning 시의 아키텍처를 조금 다르게하여 Transfer learning을 용이하게 만드는 것이 핵심입니다. 여기서 Transfer learning이란 이미 잘 훈련된 모델을 이용하여 목적에 맞게 모델을 약간 변경하여 사용하는 방법이다.


Pre-training
=============




Reference
==========

* `Minho-Park7, BERT 논문정리 <https://mino-park7.github.io/nlp/2018/12/12/bert-%EB%85%BC%EB%AC%B8%EC%A0%95%EB%A6%AC/?fbclid=IwAR3S-8iLWEVG6FGUVxoYdwQyA-zG0GpOUzVEsFBd0ARFg4eFXqCyGLznu7w>`_
